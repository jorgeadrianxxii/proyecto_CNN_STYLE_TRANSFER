{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferencia de Estilo\n",
    "## Jorge Adrián Olmos Morales.\n",
    "### jorgeadrianolmos@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos por importar las librerias necesarias\n",
    "#### (nótese la importación de la libreria pladml.keras, la cuál es una libreria para dar soporte de aceleracion hardware a computadoras AMD) [Más info](https://github.com/plaidml/plaidml) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras import backend\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion definimos los hyperparámetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ITERATIONS = 20\n",
    "CHANNELS = 3\n",
    "IMAGE_WIDTH = 350 \n",
    "IMAGE_HEIGHT = 300\n",
    "IMAGENET_MEAN_RGB_VALUES = [123.68, 116.779, 103.939]\n",
    "CONTENT_WEIGHT = .0005\n",
    "STYLE_WEIGHT = 8\n",
    "TOTAL_VARIATION_WEIGHT = 1\n",
    "TOTAL_VARIATION_LOSS_FACTOR = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los paths para nuestros archivos de entrada y salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estilo = \"path_to_foto_estilo\" # modifica este path para concordar con el path de la imagen de la cual quieres tomar el estilo\n",
    "foto = \"path_to_foto_contenido\"# modifica este path para concordar con el path de la imagen a la cual le quieres poner el estilo\n",
    "input_image_path = \"input.png\"\n",
    "style_input_path = \"style.png\"\n",
    "output_image_path = \"output.png\"\n",
    "combined_image_path = \"combined.png\"\n",
    "content_image_path = foto+\".jpeg\" # modificar dependiendo del tipo de archivo de la imagen\n",
    "style_image_path = estilo+\".jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reajustamos las dimensiones de nuestras imágenes de entrada para que puedan ser procesadas por el modelo, que en este caso es la red neuronal VGG-16 (que por cierto es un modelo ya entrenado) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_image = Image.open(content_image_path)\n",
    "input_image = input_image.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.LANCZOS)\n",
    "input_image.save(input_image_path)\n",
    "\n",
    "style_image = Image.open(style_image_path)\n",
    "style_image = style_image.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.LANCZOS)\n",
    "style_image.save(style_input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los arreglos multidimensionales que entraran en el modelo, normalizamos la información contenida en ellos y cambiamos los valores de RGB a BGR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data normalization and reshaping from RGB to BGR\n",
    "input_image_array = np.asarray(input_image, dtype=\"float32\")\n",
    "input_image_array = np.expand_dims(input_image_array, axis=0)\n",
    "input_image_array[:, :, :, 0] -= IMAGENET_MEAN_RGB_VALUES[2]\n",
    "input_image_array[:, :, :, 1] -= IMAGENET_MEAN_RGB_VALUES[1]\n",
    "input_image_array[:, :, :, 2] -= IMAGENET_MEAN_RGB_VALUES[0]\n",
    "input_image_array = input_image_array[:, :, :, ::-1]\n",
    "\n",
    "style_image_array = np.asarray(style_image, dtype=\"float32\")\n",
    "style_image_array = np.expand_dims(style_image_array, axis=0)\n",
    "style_image_array[:, :, :, 0] -= IMAGENET_MEAN_RGB_VALUES[2]\n",
    "style_image_array[:, :, :, 1] -= IMAGENET_MEAN_RGB_VALUES[1]\n",
    "style_image_array[:, :, :, 2] -= IMAGENET_MEAN_RGB_VALUES[0]\n",
    "style_image_array = style_image_array[:, :, :, ::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definimos las funciones para calcular la pérdida de contenido y la pérdida de estilo entre la imágen generada por el modelo y las imágenes de entrada. Son estas las funciones que trataremos de minimizar con el método de optimización L-BFGS. \n",
    "\n",
    "[Más info.](https://towardsdatascience.com/style-transfer-styling-images-with-convolutional-neural-networks-7d215b58f461) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_loss(content, combination):\n",
    "    return backend.sum(backend.square(combination - content))\n",
    "\n",
    "def gram_matrix(x):\n",
    "    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = backend.dot(features, backend.transpose(features))\n",
    "    return gram\n",
    "\n",
    "\n",
    "def compute_style_loss(style, combination):\n",
    "    style = gram_matrix(style)\n",
    "    combination = gram_matrix(combination)\n",
    "    size = IMAGE_HEIGHT * IMAGE_WIDTH\n",
    "    return backend.sum(backend.square(style - combination)) / (4. * (CHANNELS ** 2) * (size ** 2))\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = backend.square(x[:, :IMAGE_HEIGHT-1, :IMAGE_WIDTH-1, :] - x[:, 1:, :IMAGE_WIDTH-1, :])\n",
    "    b = backend.square(x[:, :IMAGE_HEIGHT-1, :IMAGE_WIDTH-1, :] - x[:, :IMAGE_HEIGHT-1, 1:, :])\n",
    "    return backend.sum(backend.pow(a + b, TOTAL_VARIATION_LOSS_FACTOR))\n",
    "\n",
    "def evaluate_loss_and_gradients(x):\n",
    "    x = x.reshape((1, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))\n",
    "    outs = backend.function([combination_image], outputs)([x])\n",
    "    loss = outs[0]\n",
    "    gradients = outs[1].flatten().astype(\"float64\")\n",
    "    return loss, gradients\n",
    "\n",
    "class Evaluator:\n",
    "\n",
    "    def loss(self, x):\n",
    "        loss, gradients = evaluate_loss_and_gradients(x)\n",
    "        self._gradients = gradients\n",
    "        return loss\n",
    "\n",
    "    def gradients(self, x):\n",
    "        return self._gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinamos nuestras imagenes de entrada en un tensor y alimentamos a la red con el.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_image = backend.variable(input_image_array)\n",
    "style_image = backend.variable(style_image_array)\n",
    "combination_image = backend.placeholder((1, IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "\n",
    "input_tensor = backend.concatenate([input_image,style_image,combination_image], axis=0)\n",
    "model = VGG16(input_tensor=input_tensor, include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recorremos el modelo y tomamos las activaciones generadas por el tensor de entrada en las capas deseadas (en este caso las que creemos representan el contenido y el estilo). Usaremos estas activaciones para ir calculando la pérdida global definida por \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "content_layer = \"block2_conv2\"\n",
    "layer_features = layers[content_layer]\n",
    "content_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "\n",
    "loss = backend.variable(0.)\n",
    "loss += CONTENT_WEIGHT * content_loss(content_image_features,\n",
    "                                      combination_features)\n",
    "\n",
    "\n",
    "style_layers = [\"block1_conv2\", \"block2_conv2\", \"block3_conv3\", \"block4_conv3\", \"block5_conv3\"]\n",
    "for layer_name in style_layers:\n",
    "    layer_features = layers[layer_name]\n",
    "    style_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    style_loss = compute_style_loss(style_features, combination_features)\n",
    "    loss += (STYLE_WEIGHT / len(style_layers)) * style_loss\n",
    "    \n",
    "loss += TOTAL_VARIATION_WEIGHT * total_variation_loss(combination_image)\n",
    "\n",
    "outputs = [loss]\n",
    "outputs += backend.gradients(loss, combination_image)\n",
    "\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un tensor igual a la imagen a la cual le queremos poner el estilo. Este tensor es modificado por el algoritmo L-BFGS para tratar de minimizar la funcion de error que definimos. Esto será lo que le pondra el estilo deseado a nuestra imagen.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = input_image_array\n",
    "\n",
    "x, loss, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.gradients, maxiter=ITERATIONS, iprint=5)\n",
    "print(\"Completed with loss %d\" % (loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el tensor modificado ya como imagen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x.reshape((IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))\n",
    "x = x[:, :, ::-1]\n",
    "x[:, :, 0] += IMAGENET_MEAN_RGB_VALUES[2]\n",
    "x[:, :, 1] += IMAGENET_MEAN_RGB_VALUES[1]\n",
    "x[:, :, 2] += IMAGENET_MEAN_RGB_VALUES[0]\n",
    "x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "output_image = Image.fromarray(x)\n",
    "output_image.save(output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
